# Homework 2
***
1. Please find my programming language puzzle chart [here](https://github.com/tancaotrannn/IDS2024S/blob/main/hw/2/tcaotran%20-%20progLangChartPuzzle.pdf).
- Programming languages from HW2 in chronological order (along with the decade they were developed in parentheses): FORTRAN (1950s); Algol 58 and lisp (both in 1950s); Basic (1960s); C and sh (1970s); C++ (1980s); FORTRAN 90, Java, and Python (all in 1990s); JavaScript (1990s), and C# (2000s)  
2. ENIAC stands for **E**lectronic **N**umerical **I**ntegrator **A**nd **C**omputer
3. Everything is represented by integers in computers because using binary 0 and 1 to represent data is simplistic and quick. 
4. The name of the fastest part of the computer memory is **cache memory**.
5. The slowest storage device in computers is the **hard disk drive (HDD)**.
6. A **bit** is the smallest unit of information in computer science.   
7. **Low-level language**, such as Assembly, is the closest programming language to machine code and doesn't need interpretation to be machine comprehensible.  
8. **Fortran**, or Formula Translation,  is the oldest high-level programming language created in the '50s.  
9. Assembly  
- 3rd-gen Languages: Fortran, C, C++  
- 4th-gen Languages: Python, R, Matlab  
10. C - 1970s, C++ - 1980s, Python - 1990s, MATLAB - 1980s  
11. B (programming language)  
12. C (programming language)  
13. Fortran is a programming language ancestor of Matlab and ABC (programming language) is a programming language ancestor of Python  
14. CPU register  
15. The CPU register is also the smallest memory unit.  
16. Quicker than nanoseconds  
17. Within nanoseconds compared to SSDs taking microseconds  
18. Within nanoseconds compared to HDDs taking milliseconds 
19. Transistors helped in the development of microchips as many transistors act as miniature electrical switches to turn currents on or off (binary states). Transistors also influence the power consumption of computers as transistors get smaller, leading to other performance benefits.
20. We can only add transistors to a certain amount, otherwise adding too much would take up more power and generate heat.  
21. The computer CPU cycle has an instruction fetch stage to get new instruction from the memory address, decoding stage where the encoded instruction is interpreted by the CPU decoder, and executaiton stage where the control unit of the CPU passes the decoded info to relevant function units of the CPU.
22. A powerful computer with more CPU cycles can be slower due to factors like having inefficient access to memory where there could be so much data being moved around but the memory of the computer can only handle so much data at one time, which could lead to a bottleneck or some degree of performance limitation.  
23. The bottleneck of speed can vary as it depends on the tasks and workload that a computer is undergoing, along with the hardware that it has. CPU clocks are efficient and advanced now but memory access speed hasn't kept up so bottlenecks tend to happen in memory access.
24.
- Dennard Scaling: describes the relationship between reducing the size of transistors and reducing power consumption while keeping a constant current.   
- MOSFET Scaling: is another name for Dennard Scaling; states that as transistors get smaller, their power density stays constant so that the power use stays in proportion with area.   
- Moore's Law: states that the number of transistors in microchips (or Integrated Circuits (IC)) will double bi-yearly.
25.
- 64 grains of rice
- 2080 grains of rice
- Roughly 0.297 lbs of rice 
26.
- Roughly 1.844E19 grains of rice
- Also roughly 1.844E19 grains of rice
- 2.857E15 lbs in the last square
- 1428.5 years
27. The three fundamental components are a set of primitive instructions/operations, a mechanism to execute instructions sequentially, and a mechanism for branching/conditional execution.  
28. Exponential behavior in a 2-D plot would show a curved slope that would steepen, whereas a power-law looks like a straight line on a logarithmic scale.  
